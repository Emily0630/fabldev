matches <- rep(0,P)
# Gibbs
for(s in 1:S){
AZ <- sweep(unique_patterns, MARGIN = 1, STAT = matches, FUN = "*") %>%
colSums() %>%
unname()
nonmatches <- pattern_counts - matches
BZ <- sweep(unique_patterns, MARGIN = 1, STAT = nonmatches, FUN = "*") %>%
colSums() %>%
unname()
m_post <- m_prior + AZ
u_post <- u_prior + BZ
m_post <- split(m_post, field_marker)
m <- as.vector(unlist(sapply(m_post, function(x){
prob <- MCMCpack::rdirichlet(1, x)
prob/sum(prob)
})))
u_post <- split(u_post, field_marker)
u <- as.vector(unlist(sapply(u_post, function(x){
prob <- MCMCpack::rdirichlet(1, x)
prob/sum(prob)
})))
ratio <- (log(m) - log(u)) %>%
rep(., P) %>%
matrix(., nrow = P, byrow = TRUE)
unique_weights <- exp(rowSums(ratio * unique_patterns, na.rm = TRUE))
hash_weights <- lapply(counts_by_rec, function(x){
x * unique_weights
})
pi <- rbeta(1, L + alpha, n2 - L + beta)
Z <- unname(sapply(hash_weights, function(x){
sample(candidates_P, 1, prob = c(1 - pi, x * pi / n1))
}))
#L <- sum(Z < P + 1)
L <- sum(Z > 0)
hash_matches <- factor(Z, levels = 0:P)
df <- data.frame(hash_matches)
matches <- df %>%
group_by(hash_matches, .drop = F) %>%
count() %>%
filter(hash_matches != 0) %>%
pull()
Z.SAMPS[,s] <- Z
M.SAMPS[,s] <- m
U.SAMPS[,s] <- u
L.SAMPS[s] <- L
PI.SAMPS[s] <- pi
if(show_progress){
if (s %% (S / 100) == 0) {
flush.console()
cat("\r", paste("Simulation", ": ", s / (S / 100), "% complete", sep = ""))
}
}
}
Z
n1 <- n2 <- 50
total_overlap <- n2/2
S = 50
burn = S * .1
show_progress <- T
fast = F
R <- NULL
all_patterns <- T
m <- c(.05, .95, .05, .95, .05, .95, .05, .95, .05, .95)
u <- c(.99, .01, .99, .01,
1 - 1/30, 1/30, 1 - 1/12, 1/12, 1 - 1/15, 1/15)
levels <- c(2, 2, 2, 2, 2)
possible_batches <- 1:200
pair_limit = 100000000
pair_limit = 1000
batches <- (n1 * n2 / possible_batches < pair_limit) %>%
which(. == T) %>%
.[1] %>%
possible_batches[.]
normal_batch_size <- n2 %/% batches
last_batch_size <- n2 %% batches
if(last_batch_size == 0){
last_batch_size <- NULL
}
batch_sizes <-c(rep(normal_batch_size, batches), last_batch_size)
batches_with_overlap <- total_overlap %/% normal_batch_size
remaining_overlap <- total_overlap %% normal_batch_size
overlap_vec <- rep(0, length(batch_sizes))
overlap_vec[1:batches_with_overlap] <- normal_batch_size
overlap_vec[batches_with_overlap + 1] <- remaining_overlap
hash_list <- vector("list", length(batch_sizes))
#vi_list <- vector("list", length(batch_sizes))
for (i in seq_along(batch_sizes)){
cd <- simulate_comparisons(m, u, levels, n1, batch_sizes[i], overlap_vec[i])
hash_list[[i]] <- hash_comparisons(cd, all_patterns = T)
#vi_hash_list[[i]] <- vi_hash_comparisons(cd, all_patterns = T)
print(i)
gc()
}
hash <- combine_hash(hash_list = hash_list, n1, n2)
out <- gibbs_efficient(hash, S=S, burn = burn)
result <- estimate_links(out, resolve = F)
result <- estimate_links(out)
result <- estimate_links(out$Z)
result <- estimate_links(out$Z, n1)
out$Z
Z_samps <- out$Z
# temporarily replace all nonlink labels by n1+1
Z_samps[Z_samps > n1+1] <- n1+1
# temporarily replace all nonlink labels by n1+1
#Z_samps[Z_samps > n1+1] <- n1+1
tableLabels <- apply(Z_samps, 1, tabulate, nbins=max(Z_samps))
tableLabels
tableLabels <- tableLabels/ncol(Z_samps)
tableLabels
View(tableLabels)
dim(out$Z)
devtools::load_all(".")
out <- gibbs_efficient(hash, S=S, burn = burn)
result <- estimate_links(out$Z, n1)
result$Z_hat
out <- vi_efficient(hash)
result <- vi_estimate_links(out, hash, resolve = F)
View(hash)
total_counts <- Reduce(`+`, purrr::map(hash_list, ~.x$total_counts))
pattern_counts_by_record <- hash_list %>%
purrr::map(`[[`, "pattern_counts_by_record") %>%
purrr::flatten()
hash_to_file_1 <- hash_list %>%
purrr::map(`[[`, "hash_to_file_1") %>%
purrr::flatten()
record_counts_by_pattern <- purrr::transpose(pattern_counts_by_record) %>%
purrr::map(unlist) %>%
purrr::map(unname)
hash_list[[1]]$flags
devtools::load_all(".")
for (i in seq_along(batch_sizes)){
cd <- simulate_comparisons(m, u, levels, n1, batch_sizes[i], overlap_vec[i])
hash_list[[i]] <- hash_comparisons(cd, all_patterns = T)
#vi_hash_list[[i]] <- vi_hash_comparisons(cd, all_patterns = T)
print(i)
gc()
}
hash <- combine_hash(hash_list = hash_list, n1, n2)
hash$flags
out <- vi_efficient(hash)
result <- vi_estimate_links(out, hash, resolve = F)
result$Zhat
out$t
n2 <- hash$n2
pattern_probs <- lapply(1:n2, function(j){
out$pattern_weights/out$C[j]
})
pattern_probs[[4]]
possible_records <- lapply(1:n2, function(j){
record <- c(hash$flags[[j]]$eligible_records, 0)
prob <- c(pattern_probs[[j]][hash$flags[[j]]$eligible_patterns],
exp(digamma(out$b_pi)) / out$C[j])
data.frame(record, prob)
})
possible_records[[4]]
devtools::load_all(".")
result <- vi_estimate_links(out, hash, resolve = F)
View(result$Zhat)
result$Zhat
Z_true <- rep(0, n2)
Z_true[1:total_overlap] <- 1:total_overlap
eval <- evaluate_links(result$Zhat, Z_true, n1)
ptm <- proc.time()
out <- gibbs_efficient(hash, S=S, burn = burn)
result <- estimate_links(out$Z, n1)
#eval <- evaluate_links(result$Zhat, Z_true, n1)
seconds <- (proc.time() - ptm)[3]
fabl_df <- data.frame(n1 = n1,
time = seconds,
iterations = S,
method = "fabl")
ptm <- proc.time()
out <- vi_efficient(hash)
result <- vi_estimate_links(out, hash, resolve = F)
seconds <- (proc.time() - ptm)[3]
vabl_df <- data.frame(n1 = n1,
time = seconds,
iterations = out$t,
method = "vabl")
df <- rbind(fabl_df, vabl_df)
df
# k = as.integer(Sys.getenv("SLURM_ARRAY_TASK_ID"))
# n1_seq <- seq(5000, by = 5000, length.out = 20)
# n1 <- n1_seq[k]
# n2 <- n1
n1 <- n2 <- 50
total_overlap <- n2/2
S = 50
burn = S * .1
Z_true <- rep(0, n2)
Z_true[1:total_overlap] <- 1:total_overlap
show_progress <- T
fast = F
R <- NULL
all_patterns <- T
m <- c(.05, .95, .05, .95, .05, .95, .05, .95, .05, .95)
u <- c(.99, .01, .99, .01,
1 - 1/30, 1/30, 1 - 1/12, 1/12, 1 - 1/15, 1/15)
levels <- c(2, 2, 2, 2, 2)
possible_batches <- 1:200
pair_limit = 100000000
pair_limit = 1000
batches <- (n1 * n2 / possible_batches < pair_limit) %>%
which(. == T) %>%
.[1] %>%
possible_batches[.]
normal_batch_size <- n2 %/% batches
last_batch_size <- n2 %% batches
if(last_batch_size == 0){
last_batch_size <- NULL
}
batch_sizes <-c(rep(normal_batch_size, batches), last_batch_size)
batches_with_overlap <- total_overlap %/% normal_batch_size
remaining_overlap <- total_overlap %% normal_batch_size
overlap_vec <- rep(0, length(batch_sizes))
overlap_vec[1:batches_with_overlap] <- normal_batch_size
overlap_vec[batches_with_overlap + 1] <- remaining_overlap
hash_list <- vector("list", length(batch_sizes))
#vi_list <- vector("list", length(batch_sizes))
for (i in seq_along(batch_sizes)){
cd <- simulate_comparisons(m, u, levels, n1, batch_sizes[i], overlap_vec[i])
hash_list[[i]] <- hash_comparisons(cd, all_patterns = T)
#vi_hash_list[[i]] <- vi_hash_comparisons(cd, all_patterns = T)
print(i)
gc()
}
hash <- combine_hash(hash_list = hash_list, n1, n2)
ptm <- proc.time()
out <- gibbs_efficient(hash, S=S, burn = burn)
seconds <- (proc.time() - ptm)[3]
result <- estimate_links(out$Z, n1)
#eval <- evaluate_links(result$Zhat, Z_true, n1)
fabl_df <- data.frame(n1 = n1,
time = seconds,
iterations = S,
method = "fabl")
ptm <- proc.time()
out <- vi_efficient(hash)
seconds <- (proc.time() - ptm)[3]
result <- vi_estimate_links(out, hash, resolve = F)
vabl_df <- data.frame(n1 = n1,
time = seconds,
iterations = out$t,
method = "vabl")
df <- rbind(fabl_df, vabl_df)
saveRDS(df, glue("out/speed_sim_big/n_{k}")
df
roxygen2::roxygenise().
roxygen2::roxygenise()
library(fabldev)
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
roxygen2::roxygenise()
usethis::use_vignette("troubleshooting")
library(fabldev)
getwd()
hash <- readRDS("../../vablpaper/out/big_sim_hash/n_21")
View(hash)
hash <- readRDS("../../vablpaper/out/big_sim_hash/n_21")
?hash
hash <- readRDS("../../vablpaper/out/big_sim_hash/n_21")
View(hash)
check_every <- 10
ohe <- hash$ohe # One-hot encodings e(h_p)
P <- dim(ohe)[1]
total_counts <- hash$total_counts #N_p
pattern_counts_by_record <- hash$pattern_counts_by_record #N_p_j
record_counts_by_pattern <- hash$record_counts_by_pattern
field_marker <- hash$field_marker
n1 <- hash$n1
n2 <- hash$n2
# Priors
alpha <- rep(1, length(field_marker))
Beta <- rep(1, length(field_marker))
alpha_pi <- 1
beta_pi <- 1
a <- rep(1, length(field_marker))
b <- rep(1, length(field_marker))
a_pi <- 1
b_pi <- 1
t <- 1
ratio <- 1
elbo_seq <- vector()
a_sum <- a %>%
split(., field_marker) %>%
sapply(., sum) %>%
digamma(.) %>%
.[field_marker]
a_chunk <- digamma(a) - a_sum
b_sum <- b %>%
split(., field_marker) %>%
sapply(., sum) %>%
digamma(.) %>%
.[field_marker]
b_chunk <- digamma(b) - b_sum
m_p <- ohe %>%
sweep(., 2, a_chunk, "*") %>%
rowSums()
u_p <- ohe %>%
sweep(., 2, b_chunk, "*") %>%
rowSums()
# w_p
weights = m_p - u_p
# phi_single
phi <- exp(digamma(a_pi) - digamma(n1) + weights)
single <- exp(digamma(b_pi))
# Phi_j
C <- sapply(pattern_counts_by_record, function(x){
x %*% phi + single
})
# S(Phi)
total_nonmatch <- sum(single/ C)
# N_p(Psi)
K <- sapply(1:P, function(p){
sum(record_counts_by_pattern[[p]]/C)
})
AZ <- ohe %>%
sweep(., 1, phi * K, "*") %>%
colSums()
BZ <- ohe %>%
sweep(., 1, total_counts - (phi * K), "*") %>%
colSums()
BZ <- ohe %>%
sweep(., 1, total_counts - (phi * K), "*") %>%
colSums()
total_counts - (phi * K)
ohe %>%
sweep(., 1, total_counts - (phi * K), "*")
total_counts
pattern_counts_by_record[[1]]
lookat <- sapply(hash$pattern_counts_by_record, `[`, 122)
which(is.na(lookat))
lookat
sum(lookat)
total_counts
total_counts < 2918074104
total_counts_2 <- Reduce(`+`, purrr::map(hash$pattern_counts_by_record, ~.x))
cbind(total_counts, total_counts_2) %>%
View()
total_counts_3 <- purrr::map(hash$pattern_counts_by_record, ~.x)
total_counts_3 <- purrr::map(hash$pattern_counts_by_record, ~.x) %>%
do.call(rbind, .)
total_counts_3 <- purrr::map(hash$pattern_counts_by_record, ~.x) %>%
do.call(rbind, .) %>%
colSums()
cbind(total_counts, total_counts_2, total_counts_3) %>%
View()
hash <- readRDS("../../vablpaper/out/big_sim_hash/n_21")
library(fabldev)
n1 <- 50
n2 <- 50
overlap <- n2/2
S = 50
burn = S * .1
show_progress <- T
fast = F
R <- NULL
all_patterns <- FALSE
m <- c(.05, .95, .05, .95, .05, .95, .05, .95, .05, .95)
u <- c(.99, .01, .99, .01,
1 - 1/30, 1/30, 1 - 1/12, 1/12, 1 - 1/15, 1/15)
levels <- c(2, 2, 2, 2, 2)
S <- 1000; burn <- S * .1
m_prior <- u_prior <- rep(1, length(m))
alpha <- beta <- 1
cd <- simulate_comparisons(m, u, levels, n1, n2, overlap)
Ztrue <- cd$Ztrue
comparisons <- cd
indicators <- cd[[1]]
N <- dim(indicators)[1]
n1 <- cd[[2]]
n2 <- cd[[3]]
levels <- cd[[4]]
fields <- seq_along(cd[[4]])
field_marker <- sapply(fields, function(x){
rep(x, cd[[4]][x])
}) %>%
unlist(.) %>%
as.vector(.)
ids <- expand.grid(1:n1, 1:n2)
rec1 <- ids[,1]
rec2 <- ids[,2]
Lf_vec<- (levels) %>%
c(0, .) %>%
cumsum()
hash_vals <- purrr::imap(cd[[4]], ~hash_field(.x, .y, Lf_vec)) %>%
unlist()
hash <- sweep(indicators, 2, hash_vals, "*") %>%
rowSums() + 1
if(all_patterns == TRUE){
unique_patterns <- possible_patterns_sadinle(levels)
unique_hashed <- sweep(unique_patterns, 2, hash_vals, "*") %>%
rowSums() + 1
P <- dim(unique_patterns)[1]
hash_id <- match(hash, unique_hashed) %>%
factor(levels = 1:P)
} else {
unique_hashed <- unique(hash)
P <- length(unique_hashed)
hash_id <- match(hash, unique_hashed) %>%
factor(levels = 1:P)
unique_patterns <- indicators[!duplicated(hash_id), ]
}
temp <- data.frame(indicators, rec1, rec2, hash_id)
devtools::use_r("hash_mm")
usethis::use_r("hash_mm")
temp2 <- temp %>%
group_split(rec_2)
temp2 <- temp %>%
group_split(rec2)
thing <- temp2[[1]]
thing <- temp2[[1]]
library(combinat)
thing$hash_id %>%
combn2(., 2)
500 + 725
50 * 50
thing$hash_id %>%
combn2(., 2) %>%
data.frame()
thing$hash_id %>%
combn2(., 2) %>%
data.frame() %>%
group_by(X1, X2) %>%
count()
thing$hash_id %>%
combn2(., 2)
thing$hash_id %>%
combn2(2)
thing$hash_id %>%
combn2(3)
?combn2
thing$hash_id %>%
combn(.,2)
thing$hash_id %>%
combn(.,2) %>%
data.frame()
thing$hash_id %>%
combn(2)
combn(thing$hash_id, 2)
lookat <- combn(thing$hash_id, 2)
View(lookat)
lookat <- combn(thing$hash_id, 2, simplify = F)
thing$hash_id %>%
as.numeric() %>%
combn(2)
lookat <- combn(as.numeric(thing$hash_id), 2, simplify = F)
View(lookat)
thing <- c(1, 2, 3)
combn(thing, 2)
thing <- c(1, 1, 2, 3)
combn(thing, 2)
nCm(4, 2)
nCm(50, 2)
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t()
thing <- temp2[[1]]
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t()
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t() %>%
data.frame() %>%
group_by(X1, X2) %>%
count()
?count_
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t() %>%
data.frame()
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t() %>%
data.frame() %>%
count(names(.))
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t() %>%
data.frame() %>%
count(names())
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t() %>%
data.frame() %>%
count()
thing$hash_id %>%
as.numeric() %>%
combn(2) %>%
t() %>%
data.frame() %>%
group_by_all() %>%
count()
thing$hash_id %>%
as.numeric() %>%
combn(1) %>%
t() %>%
data.frame() %>%
group_by_all() %>%
count()
expand.grid(1:P, 0:P)
expand.grid(1:P, 0:P, 0:P)
